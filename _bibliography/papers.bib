---
---

@article{zhang-et-al-2023-llmke,
  abbr = {LM-KBC},
  author = {Zhang, Bohui and Reklos, Ioannis and Jain, Nitisha and Mero単o Pe単uela, Albert and Simperl, Elena},
  title = {Using Large Language Models for Knowledge Engineering (LLMKE): A Case Study on Wikidata},
  publisher = {},
  year = {2023},
  url = {https://lm-kbc.github.io/challenge2023/},
  selected = {true},
}

@article{zhang-et-al-2023-xkgc,
  abbr = {HHAI 2023},
  doi = {10.3233/FAIA230091},
  author = {Zhang, Bohui and Mero単o Pe単uela, Albert and Simperl, Elena},
  title = {Towards Explainable Automatic Knowledge Graph Construction with Human-in-the-loop},
  abstract = {Knowledge graphs are important in human-centered AI because of their ability to reduce the need for large labelled 
              machine-learning datasets, facilitate transfer learning, and generate explanations. However, knowledge-graph construction 
              has evolved into a complex, semi-automatic process that increasingly relies on opaque deep-learning models and vast 
              collections of heterogeneous data sources to scale. The knowledge-graph lifecycle is not transparent, accountability is 
              limited, and there are no accounts of, or indeed methods to determine, how fair a knowledge graph is in the downstream 
              applications that use it. Knowledge graphs are thus at odds with AI regulation, for instance the EU's upcoming AI Act, 
              and with ongoing efforts elsewhere in AI to audit and debias data and algorithms. This paper reports on work in progress 
              towards designing explainable (XAI) knowledge-graph construction pipelines with human-in-the-loop and discusses research 
              topics in this space. These were grounded in a systematic literature review, in which we studied tasks in knowledge-graph 
              construction that are often automated, as well as common methods to explain how they work and their outcomes. We identified 
              three directions for future research: (i) tasks in knowledge-graph construction where manual input remains essential and 
              where there may be opportunities for AI assistance; (ii) integrating XAI methods into established knowledge-engineering 
              practices to improve stakeholder experience; as well as (iii) evaluating how effective explanations genuinely are in making 
              knowledge-graph construction more trustworthy.},
  booktitle = {HHAI 2023: Augmenting Human Intellect},
  publisher = {IOS Press},
  pages = {274 - 289},
  year = {2023},
  url = {https://ebooks.iospress.nl/doi/10.3233/FAIA230091},
  selected = {true},
}

@misc{zhang-et-al-2022,
  abbr = {arXiv},
  doi = {10.48550/ARXIV.2207.00143},
  author = {Zhang, Bohui and Ilievski, Filip and Szekely, Pedro},
  keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Enriching Wikidata with Linked Open Data},
  abstract = {Large public knowledge graphs, like Wikidata, contain billions of statements about tens of millions of entities, 
              thus inspiring various use cases to exploit such knowledge graphs. However, practice shows that much of the relevant 
              information that fits users' needs is still missing in Wikidata, while current linked open data (LOD) tools are not 
              suitable to enrich large graphs like Wikidata. In this paper, we investigate the potential of enriching Wikidata with 
              structured data sources from the LOD cloud. We present a novel workflow that includes gap detection, source selection, 
              schema alignment, and semantic validation. We evaluate our enrichment method with two complementary LOD sources: a noisy 
              source with broad coverage, DBpedia, and a manually curated source with a narrow focus on the art domain, Getty. Our 
              experiments show that our workflow can enrich Wikidata with millions of novel statements from external LOD sources with 
              high quality. Property alignment and data quality are key challenges, whereas entity alignment and source selection are 
              well-supported by existing Wikidata mechanisms. We make our code and data available to support future work.},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
  url = {https://arxiv.org/abs/2207.00143},
  arxiv = {2207.00143},
  selected = {true},
}
